import * as dotenv from 'dotenv';
dotenv.config(); // Load environment variables from .env file

import express from 'express';
import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai';
import { Pinecone } from '@pinecone-database/pinecone';
import { GoogleGenerativeAI } from '@google/generative-ai'; // Correct import for GoogleGenerativeAI

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware to parse JSON request bodies
app.use(express.json());

// Initialize GoogleGenerativeAI client
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// History array for the conversation.
// IMPORTANT: For a production multi-user application, this 'History'
// should be managed per user session (e.g., using a database or session store)
// to avoid cross-user conversation interference. For this example, it's global.
const conversationHistory = [];

/**
 * Transforms a user's follow-up question into a standalone query
 * using a language model, based on the current conversation history.
 * @param {string} question The user's follow-up question.
 * @returns {Promise<string>} The rephrased standalone question.
 */
async function transformQuery(question) {
  // Temporarily add the user's question to history for context
  conversationHistory.push({
    role: 'user',
    parts: [{ text: question }],
  });

  try {
    const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
    const result = await model.generateContent({
      contents: conversationHistory,
      // CORRECTED: systemInstruction is a top-level parameter
      systemInstruction: `You are a query rewriting expert. Based on the provided chat history, rephrase the "Follow Up user Question" into a complete, standalone question that can be understood without the chat history.
    Only output the rewritten question and nothing else.`,
      generationConfig: {
        // No systemInstruction here
      },
    });

    const responseText = result.response.text();
    return responseText;
  } catch (error) {
    console.error('Error transforming query:', error);
    throw new Error('Failed to transform query.');
  } finally {
    // Remove the temporary user question from history after transformation
    conversationHistory.pop();
  }
}

/**
 * Handles the core RAG (Retrieval Augmented Generation) logic.
 * It transforms the query, fetches relevant documents from Pinecone,
 * and then uses Gemini to answer the question based on the context.
 * @param {string} question The user's question.
 * @returns {Promise<string>} The answer generated by the RAG system.
 */
async function chatting(question) {
  let answer = 'I could not find the answer in the provided document.'; // Default answer

  try {
    // 1. Transform the user's question into a standalone query
    const transformedQuestion = await transformQuery(question);
    console.log('Transformed Query:', transformedQuestion);

    // 2. Embed the transformed query
    const embeddings = new GoogleGenerativeAIEmbeddings({
      apiKey: process.env.GEMINI_API_KEY,
      model: 'text-embedding-004',
    });
    const queryVector = await embeddings.embedQuery(transformedQuestion);

    // 3. Connect to Pinecone and query for relevant documents
    const pinecone = new Pinecone();
    const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX_NAME);

    const searchResults = await pineconeIndex.query({
      topK: 10,
      vector: queryVector,
      includeMetadata: true,
    });

    // 4. Extract context from search results
    const context = searchResults.matches
      .map(match => match.metadata.text)
      .join('\n\n---\n\n');

    console.log(
      'Context retrieved:',
      context.substring(0, Math.min(context.length, 200)) + '...'
    );

    // 5. Use Gemini to generate an answer based on the context
    // Add the transformed question to the conversation history for the final generation
    conversationHistory.push({
      role: 'user',
      parts: [{ text: transformedQuestion }],
    });

    const chatModel = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
    const result = await chatModel.generateContent({
      contents: conversationHistory,
      // CORRECTED: systemInstruction is a top-level parameter
      systemInstruction: `You have to behave like a Data Structure and Algorithm Expert.
      You will be given a context of relevant information and a user question.
      Your task is to answer the user's question based ONLY on the provided context.
      If the answer is not in the context, you must say "I could not find the answer in the provided document."
      Keep your answers clear, concise, and educational.

      Context: ${context}`,
    });

    answer = result.response.text();

    // Add the model's response to the conversation history
    conversationHistory.push({
      role: 'model',
      parts: [{ text: answer }],
    });

    return answer;
  } catch (error) {
    console.error('Error during RAG process:', error);
    return `An error occurred while processing your request: ${error.message}`;
  }
}

// Define the API endpoint for chat
app.post('/chat', async (req, res) => {
  const { question } = req.body;

  if (!question) {
    return res
      .status(400)
      .json({ error: 'Question is required in the request body.' });
  }

  try {
    const ragAnswer = await chatting(question);
    res.json({ answer: ragAnswer });
  } catch (error) {
    console.error('API Error:', error);
    res
      .status(500)
      .json({ error: 'Internal server error during chat processing.' });
  }
});

// Start the server
app.listen(PORT, () => {
  console.log(`Server is running on http://localhost:${PORT}`);
  console.log('API endpoint: POST /chat');
  console.log(
    'Send a JSON body like: { "question": "What is a binary tree?" }'
  );
});
