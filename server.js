import * as dotenv from 'dotenv';
dotenv.config(); // Load environment variables from .env file

import express from 'express';
import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai';
import { Pinecone } from '@pinecone-database/pinecone';
import { GoogleGenerativeAI } from '@google/generative-ai'; // Correct import for GoogleGenerativeAI

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware to parse JSON request bodies
app.use(express.json());

// Initialize GoogleGenerativeAI client
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

/**
 * Handles the core RAG (Retrieval Augmented Generation) logic.
 * It embeds the user's question, fetches relevant documents from Pinecone,
 * and then uses Gemini to answer the question based on the context.
 * This version does NOT maintain conversation history.
 * @param {string} question The user's question.
 * @returns {Promise<string>} The answer generated by the RAG system.
 */
async function chatting(question) {
  let answer = 'I could not find the answer in the provided document.'; // Default answer

  try {
    // 1. Embed the user's question directly (no transformation needed without history)
    const embeddings = new GoogleGenerativeAIEmbeddings({
      apiKey: process.env.GEMINI_API_KEY,
      model: 'text-embedding-004',
    });
    const queryVector = await embeddings.embedQuery(question); // Use original question

    // 2. Connect to Pinecone and query for relevant documents
    const pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY,
      controllerHostUrl: process.env.PINECONE_CONTROLLER_HOST_URL,
    });
    const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX_NAME);

    const searchResults = await pineconeIndex.query({
      topK: 10,
      vector: queryVector,
      includeMetadata: true,
    });

    // 3. Extract context from search results
    const context = searchResults.matches
      .map(match => match.metadata.text)
      .join('\n\n---\n\n');

    console.log(
      'Context retrieved:',
      context.substring(0, Math.min(context.length, 200)) + '...'
    );

    // 4. Use Gemini to generate an answer based on the context
    const chatModel = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

    const result = await chatModel.generateContent({
      // Contents now just include the single user turn
      contents: [{ role: 'user', parts: [{ text: question }] }],
      systemInstruction: `You have to behave like a Data Structure and Algorithm Expert.
      You will be given a context of relevant information and a user question.
      Your task is to answer the user's question based ONLY on the provided context.
      If the answer is not in the context, you must say "I could not find the answer in the provided document."
      Keep your answers clear, concise, and educational.

      Context: ${context}`,
    });

    answer = result.response.text();

    return answer;
  } catch (error) {
    console.error('Error during RAG process:', error);
    return `An error occurred while processing your request: ${error.message}`;
  }
}

// Define the API endpoint for chat
app.post('/chat', async (req, res) => {
  const { question } = req.body; // No longer expecting conversationId

  if (!question) {
    return res
      .status(400)
      .json({ error: 'Question is required in the request body.' });
  }

  try {
    const ragAnswer = await chatting(question);
    res.json({ answer: ragAnswer }); // No longer returning conversationId
  } catch (error) {
    console.error('API Error:', error);
    res
      .status(500)
      .json({ error: 'Internal server error during chat processing.' });
  }
});

// Start the server
app.listen(PORT, () => {
  console.log(`Server is running on http://localhost:${PORT}`);
  console.log('API endpoint: POST /chat');
  console.log(
    'Send a JSON body like: { "question": "What is a binary tree?" }'
  );
});
