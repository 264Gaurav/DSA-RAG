import * as dotenv from 'dotenv';
dotenv.config(); // Load environment variables from .env file

import express from 'express';
import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai';
import { Pinecone } from '@pinecone-database/pinecone';
import { GoogleGenerativeAI } from '@google/generative-ai'; // Correct import for GoogleGenerativeAI

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware to parse JSON request bodies
app.use(express.json());

// Initialize GoogleGenerativeAI client
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

/**
 * Transforms a user's question into a complete, standalone query.
 * This function uses a temporary, isolated chat history for the transformation
 * to ensure the rephrased question is coherent, but does not maintain
 * a persistent conversation history across API calls.
 * @param {string} question The user's original question.
 * @returns {Promise<string>} The rephrased standalone question.
 */
async function transformQuery(question) {
  // Create a temporary history for the query transformation model.
  // This ensures the transformation is context-aware for the current turn,
  // but doesn't persist across different API requests.
  const tempQueryTransformHistory = [];

  // Add the user's question to the temporary history
  tempQueryTransformHistory.push({
    role: 'user',
    parts: [{ text: question }],
  });

  try {
    const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
    const result = await model.generateContent({
      contents: tempQueryTransformHistory, // Use the temporary history for transformation
      systemInstruction: `You are a query rewriting expert. Based on the provided chat history, rephrase the "Follow Up user Question" into a complete, standalone question that can be understood without the chat history.
    Only output the rewritten question and nothing else.`,
    });

    const responseText = result.response.text();
    return responseText;
  } catch (error) {
    console.error('Error transforming query:', error);
    // Re-throw or handle as appropriate for your application
    throw new Error('Failed to transform query.');
  }
}

/**
 * Handles the core RAG (Retrieval Augmented Generation) logic.
 * It first transforms the user's question, then embeds the transformed query,
 * fetches relevant documents from Pinecone, and finally uses Gemini to
 * answer the question based on the retrieved context.
 * @param {string} question The user's original question.
 * @returns {Promise<string>} The answer generated by the RAG system.
 */
async function chatting(question) {
  let answer = 'I could not find the answer in the provided document.'; // Default answer

  try {
    // STEP 1: Modify and structure the question correctly
    const transformedQuestion = await transformQuery(question);
    console.log('Original Question:', question);
    console.log('Transformed Query:', transformedQuestion);

    // STEP 2: Embed the transformed query
    const embeddings = new GoogleGenerativeAIEmbeddings({
      apiKey: process.env.GEMINI_API_KEY,
      model: 'text-embedding-004',
    });
    const queryVector = await embeddings.embedQuery(transformedQuestion);

    // STEP 3: Connect to Pinecone and query for relevant documents
    const pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY,
      controllerHostUrl: process.env.PINECONE_CONTROLLER_HOST_URL,
    });
    const pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX_NAME);

    const searchResults = await pineconeIndex.query({
      topK: 10,
      vector: queryVector,
      includeMetadata: true,
    });

    // STEP 4: Extract context from search results
    const context = searchResults.matches
      .map(match => match.metadata.text)
      .join('\n\n---\n\n');

    console.log(
      'Context retrieved:',
      context.substring(0, Math.min(context.length, 200)) + '...'
    );

    // STEP 5: Use Gemini to generate an answer based on the context
    const chatModel = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

    const result = await chatModel.generateContent({
      // The contents for the RAG model now include the transformed question
      contents: [{ role: 'user', parts: [{ text: transformedQuestion }] }],
      systemInstruction: `You have to behave like a Data Structure and Algorithm Expert.
      You will be given a context of relevant information and a user question.
      Your task is to answer the user's question based ONLY on the provided context.
      If the answer is not in the context, you must say "I could not find the answer in the provided document."
      Keep your answers clear, concise, and educational.

      Context: ${context}`,
    });

    answer = result.response.text();

    return answer;
  } catch (error) {
    console.error('Error during RAG process:', error);
    return `An error occurred while processing your request: ${error.message}`;
  }
}

// Define the API endpoint for chat
app.post('/chat', async (req, res) => {
  const { question } = req.body;

  if (!question) {
    return res
      .status(400)
      .json({ error: 'Question is required in the request body.' });
  }

  try {
    const ragAnswer = await chatting(question);
    res.json({ answer: ragAnswer });
  } catch (error) {
    console.error('API Error:', error);
    res
      .status(500)
      .json({ error: 'Internal server error during chat processing.' });
  }
});

// Start the server
app.listen(PORT, () => {
  console.log(`Server is running on http://localhost:${PORT}`);
  console.log('API endpoint: POST /chat');
  console.log(
    'Send a JSON body like: { "question": "What is a binary tree?" }'
  );
});
